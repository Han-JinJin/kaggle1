"""
Utility functions for Vesuvius Ink Detection
Data loading, augmentation, and helper functions
"""

import glob
import os
import cv2
from PIL import Image, ImageOps
import numpy as np
from scipy import ndimage
from tqdm import tqdm
import random
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch
import torch.nn.functional as F
from scipy.interpolate import CubicSpline
import traceback


def create_lut(control_points, bit_depth=8):
    """
    Create a lookup table (LUT) based on control points for the given bit depth.

    Args:
        control_points (list of tuples): Control points in 8-bit range, e.g. [(0,0), (128,64), (255,255)]
        bit_depth (int): Bit depth of the image (8 or 16)

    Returns:
        np.ndarray: The generated LUT as a 1D numpy array.
    """
    if bit_depth == 16:
        # Scale control points from 8-bit to 16-bit (0-255 -> 0-65535)
        control_points = [(x * 257, y * 257) for x, y in control_points]
        x_points, y_points = zip(*control_points)
        x_range = np.linspace(0, 65535, 65536)
        spline = CubicSpline(x_points, y_points)
        lut = spline(x_range)
        lut = np.clip(lut, 0, 65535).astype('uint16')
    elif bit_depth == 8:
        x_points, y_points = zip(*control_points)
        x_range = np.linspace(0, 255, 256)
        spline = CubicSpline(x_points, y_points)
        lut = spline(x_range)
        lut = np.clip(lut, 0, 255).astype('uint8')
    else:
        raise ValueError("Unsupported bit depth: {}".format(bit_depth))
    return lut


def apply_lut_to_stack(img_stack, lut):
    """
    Apply a LUT to a 3D grayscale image stack (H, W, D).

    Args:
        img_stack (np.ndarray): Input stack with shape (H, W, D), dtype uint8.
        lut (np.ndarray): Lookup table of shape (256,) for 8-bit images.

    Returns:
        np.ndarray: Contrast-adjusted image stack.
    """
    if img_stack.dtype != np.uint8:
        raise ValueError("Image stack must be of dtype uint8.")
    if lut.shape[0] != 256:
        raise ValueError("LUT must have 256 values for 8-bit images.")

    # Apply LUT using NumPy fancy indexing â€” fast and vectorized
    return lut[img_stack]


def read_image_mask(fragment_id, CFG=None):
    """
    Reads a fragment image and its corresponding masks.

    Args:
        fragment_id: Fragment identifier
        CFG: Configuration object with parameters:
            - segment_path: Path to segment directory
            - start_idx: Starting layer index
            - in_chans: Number of input channels
            - valid_chans: Number of validation channels
            - valid_id: Validation fragment ID
            - size: Tile size
            - frags_ratio1, frags_ratio2: Fragment scaling identifiers
            - ratio1, ratio2: Scaling ratios

    Returns:
        images: (H, W, in_chans) numpy array
        mask: (H, W) float32 array, ink labels normalized to [0, 1]
        fragment_mask: (H, W) float32 array, fragment mask normalized to [0, 1]
    """

    control_points = [(0, 0), (128, 64), (255, 255)]

    # Create the LUT
    lut = create_lut(control_points, bit_depth=8)

    images = []
    start_idx = CFG.start_idx
    end_idx = start_idx + CFG.in_chans

    idxs = range(start_idx, end_idx)
    image_shape = 0

    # For validation, adjust to use middle valid_chans
    if fragment_id == CFG.valid_id:
        print('valid fragment detected, adjusting channels')
        start_idx = int(start_idx + (CFG.in_chans - CFG.valid_chans) // 2)
        end_idx = start_idx + CFG.valid_chans
        idxs = range(start_idx, end_idx)
        print(f"Using layers {start_idx}-{end_idx-1} for validation")

    try:
        for i in tqdm(idxs, desc=f"Loading {fragment_id}"):
            tif_path = os.path.join(CFG.segment_path, fragment_id, "layers", f"{i:02}.tif")
            jpg_path = os.path.join(CFG.segment_path, fragment_id, "layers", f"{i:02}.jpg")
            png_path = os.path.join(CFG.segment_path, fragment_id, "layers", f"{i:02}.png")

            # Try to load image from different formats
            if os.path.exists(tif_path):
                image = cv2.imread(tif_path, 0)
            elif os.path.exists(jpg_path):
                image = cv2.imread(jpg_path, 0)
            else:
                image = cv2.imread(png_path, 0)

            # Resize the image based on fragment type
            if (any(sub in fragment_id for sub in CFG.frags_ratio1)):
                scale = 1 / CFG.ratio1
                new_w = int(image.shape[1] * scale)
                new_h = int(image.shape[0] * scale)
                image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
            elif (any(sub in fragment_id for sub in CFG.frags_ratio2)):
                scale = 1 / CFG.ratio2
                new_w = int(image.shape[1] * scale)
                new_h = int(image.shape[0] * scale)
                image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)

            image_shape = (image.shape[1], image.shape[0])

            # Clip pixel values to reduce noise
            image = np.clip(image, 0, 200)
            images.append(image)

        # Stack all channels
        images = np.stack(images, axis=2)
        print(f"Shape of {fragment_id} segment: {images.shape}")

        # Initialize masks
        mask = np.zeros(images.shape[:2], dtype=np.uint8)  # (H, W)
        fragment_mask = np.zeros(images.shape[:2], dtype=np.uint8)  # (H, W)

        # READ INK LABELS
        inklabel_files = glob.glob(f"{CFG.segment_path}/{fragment_id}/*inklabels.*")

        if len(inklabel_files) > 0:
            mask = cv2.imread(inklabel_files[0], 0)
        else:
            # Create empty mask if no inklabels file exists
            print(f"No inklabels found for {fragment_id}, creating empty mask")
            mask = np.zeros(images.shape[:2], dtype=np.uint8)

            # Save empty mask
            save_dir = f"{CFG.segment_path}/{fragment_id}"
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"{fragment_id}_inklabels.png")
            cv2.imwrite(save_path, mask)
            print(f"Saved empty mask to: {save_path}")

        # Resize mask to match image size
        mask = cv2.resize(mask, image_shape, interpolation=cv2.INTER_AREA)

        # Read fragment mask
        path = f"{CFG.segment_path}{fragment_id}/{fragment_id}_mask.png"
        if os.path.exists(path):
            fragment_mask = cv2.imread(path, 0)
            fragment_mask = cv2.resize(fragment_mask, image_shape, interpolation=cv2.INTER_AREA)
        else:
            print(f"Warning: Fragment mask not found at {path}")
            fragment_mask = np.ones(image_shape, dtype=np.uint8) * 255

        # Normalize masks to [0, 1]
        mask = mask.astype('float32') / 255.0
        fragment_mask = fragment_mask.astype('float32') / 255.0

    except Exception as e:
        print(f"Error processing fragment {fragment_id}: {e}")
        traceback.print_exc()
        return None, None, None

    return images, mask, fragment_mask


def get_train_valid_dataset(CFG=None):
    """
    Extract training and validation tiles from fragments.

    Returns:
        train_images: list of (size, size, in_chans) tiles
        train_masks: list of (size, size, 1) masks
        valid_images: list of (size, size, valid_chans) tiles
        valid_masks: list of (size, size, 1) masks
        valid_xyxys: list of [x1, y1, x2, y2] coordinates for validation
    """
    train_images = []
    train_masks = []

    valid_images = []
    valid_masks = []
    valid_xyxys = []

    segments = CFG.segments
    path = CFG.segment_path

    for fragment_id in segments:
        fragment_path = os.path.join(path, fragment_id)

        if not os.path.isdir(fragment_path):
            print(f"Warning: Fragment directory not found: {fragment_path}")
            continue

        print(f'Reading fragment: {fragment_id}')

        image, mask, fragment_mask = read_image_mask(fragment_id, CFG=CFG)

        if image is None or mask is None or fragment_mask is None:
            print(f"Skipping {fragment_id} due to loading error")
            continue

        # Calculate tile positions
        x1_list = list(range(0, image.shape[1] - CFG.tile_size + 1, CFG.stride))
        y1_list = list(range(0, image.shape[0] - CFG.tile_size + 1, CFG.stride))

        print(f"Extracting tiles from {len(x1_list) * len(y1_list)} windows...")

        windows_dict = {}

        for a in y1_list:
            for b in x1_list:
                # Check if tile has sufficient fragment coverage
                if np.mean(fragment_mask[a:a + CFG.tile_size, b:b + CFG.tile_size]) >= 1:

                    # For validation, ensure some ink labels exist
                    if fragment_id == CFG.valid_id or not np.all(mask[a:a + CFG.tile_size, b:b + CFG.tile_size] < 0.05):

                        # Extract size x size tiles within tile_size window
                        for yi in range(0, CFG.tile_size, CFG.size):
                            for xi in range(0, CFG.tile_size, CFG.size):
                                y1 = a + yi
                                x1 = b + xi
                                y2 = y1 + CFG.size
                                x2 = x1 + CFG.size

                                # Avoid duplicate tiles
                                if (y1, y2, x1, x2) in windows_dict:
                                    continue
                                windows_dict[(y1, y2, x1, x2)] = '1'

                                if fragment_id != CFG.valid_id:
                                    # Training tile
                                    train_images.append(image[y1:y2, x1:x2])
                                    train_masks.append(mask[y1:y2, x1:x2, None])
                                    assert image[y1:y2, x1:x2].shape == (CFG.size, CFG.size, CFG.in_chans), \
                                        f"Unexpected shape: {image[y1:y2, x1:x2].shape}"
                                else:
                                    # Validation tile
                                    valid_images.append(image[y1:y2, x1:x2])
                                    valid_masks.append(mask[y1:y2, x1:x2, None])
                                    valid_xyxys.append([x1, y1, x2, y2])
                                    assert image[y1:y2, x1:x2].shape == (CFG.size, CFG.size, CFG.valid_chans), \
                                        f"Unexpected shape: {image[y1:y2, x1:x2].shape}"

        print(f"Extracted {len([x for x in windows_dict if (x[0] + x[1]) % CFG.size == 0])} unique tiles")

    print(f"\nTotal train images: {len(train_images)}")
    print(f"Total valid images: {len(valid_images)}")

    return train_images, train_masks, valid_images, valid_masks, valid_xyxys


# ============================================================================
# Video Dataset
# ============================================================================

import torch
from torch.utils.data import Dataset
import torchvision.transforms as T


class VideoDataset(Dataset):
    """
    Video dataset for 3D volumetric data.

    Args:
        images: list of numpy arrays with shape (H, W, C)
        cfg: configuration object
        xyxys: list of [x1, y1, x2, y2] for validation (optional)
        labels: list of masks (optional)
        transform: albumentations transform
        norm: whether to normalize
        aug: augmentation type ('fourth', 'shuffle', or None)
        out_chans: number of output channels
        scale_factor: downscale factor for mask
    """

    def __init__(
        self,
        images,
        cfg,
        xyxys=None,
        labels=None,
        transform=None,
        norm=True,
        aug=None,
        out_chans=1,
        scale_factor=4  # FIXED: Changed from 8 to 4 to match SwinUNet3D output (1/4 resolution)
    ):
        self.images = images
        self.cfg = cfg
        self.labels = labels
        self.transform = transform
        self.xyxys = xyxys
        self.aug = aug
        self.out_chans = out_chans
        self.scale_factor = scale_factor

        # ---------------------- #
        #   MAIN VIDEO TRANSFORM
        # ---------------------- #
        t_list = [T.ConvertImageDtype(torch.float32)]

        if norm:
            out_ch = self.out_chans

            if out_ch == 3:
                # Standard RGB ImageNet normalization
                mean = [0.485, 0.456, 0.406]
                std  = [0.229, 0.224, 0.225]
            elif out_ch == 1:
                # Grayscale normalization
                mean = [0.5]
                std  = [0.5]
            else:
                # Generic N-channel normalization
                mean = [0.5] * out_ch
                std  = [0.5] * out_ch

            t_list.append(T.Normalize(mean=mean, std=std))

        self.video_transform = T.Compose(t_list)

    def __len__(self):
        return len(self.images)

    # ------------------------------------------------------------ #
    #                      AUGMENTATIONS
    # ------------------------------------------------------------ #
    def fourth_augment(self, image):
        """
        Randomly crop K contiguous channels from in_chans channels.
        This simulates the MAE pretraining augmentation.
        """
        C = self.cfg.in_chans
        K = self.cfg.valid_chans

        # Edge case: if C == K, no cropping needed
        if C == K:
            return image

        # Pick contiguous crop
        start_idx = random.randint(0, C - K)
        crop_idx = np.arange(start_idx, start_idx + K)
        cropped = image[..., crop_idx].copy()

        # Randomly zero out some channels (3% probability)
        zero_mask = np.random.rand(K) < 0.03
        cropped[..., zero_mask] = 0

        return cropped

    def shuffle_channels(self, image):
        """Random channel shuffle."""
        K = self.cfg.valid_chans
        perm = np.random.permutation(K)
        return image[..., perm]

    # ------------------------------------------------------------ #
    #                 APPLY AUGMENTATION CHOICE
    # ------------------------------------------------------------ #

    def apply_aug(self, image):
        """Apply selected augmentation."""
        if self.aug == "fourth":
            image = self.fourth_augment(image)
        elif self.aug == "shuffle":
            image = self.shuffle_channels(image)
        return image

    # ------------------------------------------------------------ #
    #                      MAIN GETITEM
    # ------------------------------------------------------------ #

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx] if self.labels is not None else None

        # VALID CASE
        if self.xyxys is not None:
            xy = self.xyxys[idx]

            if self.transform:
                data = self.transform(image=image, mask=label)
                image = data["image"].unsqueeze(0)
                label = data["mask"]

                # Downsample label to match model output (1/4 resolution)
                label = F.interpolate(
                    label.unsqueeze(0),
                    (self.cfg.size // self.scale_factor, self.cfg.size // self.scale_factor)
                ).squeeze(0)

            # Permute to [frames, C, H, W]
            image = image.permute(1, 0, 2, 3)  # (T, 1, H, W)

            # Convert and normalize frames
            image = torch.stack([self.video_transform(f) for f in image])  # (T, 1, H, W)

            # Repeat channels if needed
            if image.shape[1] != self.out_chans:
                image = image.repeat(1, self.out_chans, 1, 1)

            # CRITICAL FIX: Permute from (T, 1, H, W) to (1, T, H, W) for Swin3D
            # After DataLoader: (B, 1, T, H, W) which matches Swin3D input format
            image = image.permute(1, 0, 2, 3)  # (1, T, H, W)

            return image, label, xy

        # TRAIN CASE
        else:
            # Apply augmentation
            image = self.apply_aug(image)

            # Apply albumentations
            if self.transform:
                data = self.transform(image=image, mask=label)
                image = data["image"].unsqueeze(0)
                label = data["mask"]

                # Downsample label
                label = F.interpolate(
                    label.unsqueeze(0),
                    (self.cfg.size // self.scale_factor, self.cfg.size // self.scale_factor)
                ).squeeze(0)

            # Permute to video frames
            image = image.permute(1, 0, 2, 3)  # (T, 1, H, W)

            # Apply transforms frame-wise
            image = torch.stack([self.video_transform(f) for f in image])  # (T, 1, H, W)

            # Repeat channels if needed (though out_chans=1 usually)
            if image.shape[1] != self.out_chans:
                image = image.repeat(1, self.out_chans, 1, 1)

            # CRITICAL FIX: Permute from (T, 1, H, W) to (1, T, H, W) for Swin3D
            # After DataLoader: (B, 1, T, H, W) which matches Swin3D input format
            image = image.permute(1, 0, 2, 3)  # (1, T, H, W)

            return image, label


# ============================================================================
# Helper Functions
# ============================================================================

def get_transforms(data, cfg):
    """Get data transforms based on split type."""
    if data == 'train':
        aug = A.Compose(cfg.train_aug_list)
    elif data == 'valid':
        aug = A.Compose(cfg.valid_aug_list)
    return aug


def set_seed(seed=None, cudnn_deterministic=True):
    """Set random seed for reproducibility."""
    if seed is None:
        seed = 42
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = cudnn_deterministic
    torch.backends.cudnn.benchmark = False


def make_dirs(cfg):
    """Create necessary directories."""
    for dir in [cfg.model_dir]:
        os.makedirs(dir, exist_ok=True)


def cfg_init(cfg, mode='train'):
    """Initialize configuration."""
    set_seed(cfg.seed)
    if mode == 'train':
        make_dirs(cfg)
