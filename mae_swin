"""
MAE (Masked Autoencoder) Pretraining for Vesuvius Challenge
Uses torchvision Swin3D transformer
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from warmup_scheduler import GradualWarmupScheduler
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset, random_split
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torchvision.transforms as T
import random
from tqdm.auto import tqdm

# Use torchvision's Swin3D transformer instead of custom implementation
from torchvision.models.video import swin_transformer


PIL_Image_MAX_IMAGE_PIXELS = 933120000


class CFG:
    # ============== comp exp name =============
    current_dir = '../'
    segment_path = './pretraining_scrolls/'

    # Data parameters - FIXED: Match with data-prepare and train
    start_idx = 20
    in_chans = 20
    valid_chans = 16  # Randomly crop to 16 channels for training

    size = 224
    tile_size = 224
    stride = tile_size // 1  # No overlap for MAE pretraining

    # Training parameters
    train_batch_size = 10
    valid_batch_size = 5
    lr = 1e-4

    # ============== model cfg =============
    scheduler = 'cosine'
    epochs = 16  # Reduced from 200 for faster iteration
    warmup_factor = 10

    # Fragment scaling - FIXED: ratio2=2 to match data-prepare
    frags_ratio1 = ['frag', 're']
    frags_ratio2 = ['202', 's4', 'left']
    ratio1 = 2
    ratio2 = 2  # FIXED: Changed from 1 to 2

    # ============== valid =============
    segments = ['20240304141531']
    valid_id = '20240304141531'

    # ============== fixed =============
    min_lr = 1e-7
    weight_decay = 1e-6
    max_grad_norm = 100
    num_workers = 16
    seed = 0

    # ============== augmentation =============
    train_aug_list = [
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.ShiftScaleRotate(rotate_limit=360, shift_limit=0.15, p=0.75),
        ToTensorV2(transpose_mask=True),
    ]

    valid_aug_list = [
        ToTensorV2(transpose_mask=True),
    ]


def get_transforms(data, cfg):
    """Get data transforms based on split type"""
    if data == 'train':
        aug = A.Compose(cfg.train_aug_list)
    elif data == 'valid':
        aug = A.Compose(cfg.valid_aug_list)
    return aug


class TileDataset(Dataset):
    """Dataset for loading preprocessed .npy tiles"""
    def __init__(self, base_path, splits=["train"], transform=None):
        self.tile_paths = []
        for split in splits:
            split_path = os.path.join(base_path, "224_tiles", split)
            self.tile_paths += [
                os.path.join(split_path, f)
                for f in os.listdir(split_path) if f.endswith(".npy")
            ]

        self.transform = transform
        self.video_transform = T.Compose([
            T.ConvertImageDtype(torch.float32),
            T.Normalize(mean=[0.5], std=[0.5])
        ])

    def __len__(self):
        return len(self.tile_paths)

    def fourth_augment(self, image):
        """
        Randomly crop 16 contiguous channels from 20 channels.
        This simulates the same augmentation used in main training.
        """
        cropping_num = CFG.valid_chans  # 16
        start_idx = random.randint(0, CFG.in_chans - cropping_num)
        crop_indices = np.arange(start_idx, start_idx + cropping_num)
        return image[..., crop_indices]

    def __getitem__(self, idx):
        image = np.load(self.tile_paths[idx])  # (224, 224, 20)
        image = self.fourth_augment(image)  # (224, 224, 16)

        if self.transform:
            data = self.transform(image=image)
            image = data['image'].unsqueeze(0)
        image = image.permute(1, 0, 2, 3)  # (16, 224, 224)
        image = torch.stack([self.video_transform(f) for f in image])
        return image


def visualize_reconstruction(original, reconstructed, sample_idx=0, num_frames=16,
                            save_path='./results/reconstruction.png', epoch=0):
    """Visualize original and reconstructed video frames side by side."""
    orig = original[sample_idx]  # (T, C, H, W)
    recon = reconstructed[sample_idx]  # (T, C, H, W)

    # If grayscale, squeeze channel dim
    if orig.shape[1] == 1:
        orig = orig.squeeze(1)
        recon = recon.squeeze(1)

    # Clamp and convert to numpy
    orig = orig.cpu().numpy()
    recon = recon.cpu().detach().numpy()

    fig, axes = plt.subplots(2, num_frames, figsize=(3 * num_frames, 6))

    for i in range(num_frames):
        # Original frame
        ax = axes[0, i]
        ax.imshow(orig[i], cmap='gray')
        ax.set_title(f"Original Frame {i}")
        ax.axis('off')

        # Reconstructed frame
        ax = axes[1, i]
        ax.imshow(recon[i], cmap='gray')
        ax.set_title(f"Reconstructed Frame {i}")
        ax.axis('off')

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close(fig)


class Swin3DMAE(pl.LightningModule):
    """
    3D Video MAE using torchvision's Swin3D Transformer

    Uses a simpler approach:
    - Encoder: Swin3D from torchvision
    - Decoder: Lightweight Transformer decoder
    - Masking: 75% random masking
    """
    def __init__(self,
                 img_size=(16, 224, 224),
                 patch_size=(2, 4, 4),
                 in_channels=1,
                 embed_dim=96,
                 depths=(2, 2, 6, 2),
                 num_heads=(3, 6, 12, 24),
                 window_size=(4, 7, 7),
                 decoder_embed_dim=512,
                 decoder_depth=4,
                 decoder_num_heads=8,
                 mask_ratio=0.75,
                 lr=1e-4):
        super().__init__()
        self.save_hyperparameters()

        self.img_size = img_size  # (T, H, W)
        self.patch_size = patch_size  # (pt, ph, pw)
        self.mask_ratio = mask_ratio
        self.in_channels = in_channels

        # Calculate patch grid
        self.patch_grid = (
            img_size[0] // patch_size[0],  # T / pt
            img_size[1] // patch_size[1],  # H / ph
            img_size[2] // patch_size[2]   # W / pw
        )
        self.num_patches = self.patch_grid[0] * self.patch_grid[1] * self.patch_grid[2]
        print(f"Patch grid: {self.patch_grid}, Total patches: {self.num_patches}")

        # ========== ENCODER: Swin3D from torchvision ==========
        # Note: We use the backbone without the classification head
        self.encoder = swin_transformer.swin3d_t(weights='KINETICS400_V1')

        # Modify first conv layer to accept 1 channel instead of 3
        old_proj = self.encoder.patch_embed.proj
        new_proj = nn.Conv3d(
            in_channels=1,
            out_channels=old_proj.out_channels,
            kernel_size=old_proj.kernel_size,
            stride=old_proj.stride,
            padding=old_proj.padding,
            bias=(old_proj.bias is not None)
        )
        # Initialize weights by summing across RGB channels
        with torch.no_grad():
            summed = old_proj.weight.sum(dim=1, keepdim=True)  # [out_channels, 1, kT, kH, kW]
            new_proj.weight.copy_(summed)
            if old_proj.bias is not None:
                new_proj.bias.copy_(old_proj.bias)

        self.encoder.patch_embed.proj = new_proj

        # Remove classification head
        self.encoder.head = nn.Identity()

        # Get encoder output dimension
        self.encoder_dim = 768  # Swin3D-tiny final dimension

        # ========== DECODER ==========
        self.decoder_embed = nn.Linear(self.encoder_dim, decoder_embed_dim)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))

        # Positional embedding for full sequence
        self.decoder_pos_embed = nn.Parameter(
            torch.zeros(1, self.num_patches, decoder_embed_dim)
        )

        # Transformer decoder
        decoder_layer = nn.TransformerEncoderLayer(
            d_model=decoder_embed_dim,
            nhead=decoder_num_heads,
            dim_feedforward=decoder_embed_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_depth)

        # Prediction head
        patch_dim = patch_size[0] * patch_size[1] * patch_size[2] * in_channels
        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)

        # Loss
        self.criterion = nn.MSELoss()

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        nn.init.normal_(self.mask_token, std=0.02)
        nn.init.normal_(self.decoder_pos_embed, std=0.02)
        nn.init.normal_(self.decoder_pred.weight, std=0.02)
        nn.init.constant_(self.decoder_pred.bias, 0)

    def patchify(self, x):
        """
        Convert video to patches.
        x: (B, C, T, H, W)
        Returns: (B, num_patches, patch_dim)
        """
        B, C, T, H, W = x.shape
        pt, ph, pw = self.patch_size

        # Reshape to patches
        x = x.reshape(B, C,
                     T // pt, pt,
                     H // ph, ph,
                     W // pw, pw)
        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()
        x = x.reshape(B, -1, C * pt * ph * pw)
        return x

    def unpatchify(self, x):
        """
        Convert patches back to video.
        x: (B, num_patches, patch_dim)
        Returns: (B, C, T, H, W)
        """
        B = x.shape[0]
        pt, ph, pw = self.patch_size
        T, H, W = self.img_size
        gt, gh, gw = self.patch_grid

        x = x.reshape(B, gt, gh, gw, self.in_channels, pt, ph, pw)
        x = x.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()
        x = x.reshape(B, self.in_channels, T, H, W)
        return x

    def random_masking(self, x, mask_ratio):
        """
        Random masking following MAE.
        x: (B, N, D)
        Returns:
            x_masked: (B, N_keep, D)
            mask: (B, N) binary mask (1 = masked, 0 = keep)
            ids_restore: (B, N) to restore original order
        """
        B, N, D = x.shape
        len_keep = int(N * (1 - mask_ratio))

        noise = torch.rand(B, N, device=x.device)
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        ids_keep = ids_shuffle[:, :len_keep]
        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))

        mask = torch.ones([B, N], device=x.device)
        mask[:, :len_keep] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return x_masked, mask, ids_restore

    def forward_encoder(self, x, mask_ratio):
        """
        Forward through encoder with masking.
        x: (B, T, C, H, W)
        For Swin3D compatibility, we pass the full input through the encoder,
        then apply masking to the output features.
        """
        B = x.shape[0]

        # Handle input format
        if x.dim() == 5 and x.shape[2] == 1:  # (B, T, C, H, W) with C=1
            x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)

        # Patchify for target
        x_patches = self.patchify(x)  # (B, N, patch_dim)

        # Forward through encoder (full input)
        # FIXED: Use correct torchvision Swin3D forward pass
        # torchvision Swin3D doesn't have forward_features, use standard forward
        # but we need to get features before the head

        # Get patch embedding
        x = self.encoder.patch_embed(x)  # (B, L, embed_dim)
        x = self.encoder.pos_drop(x)

        # Pass through transformer stages
        for block in self.encoder.features:
            x = block(x)

        # Global average pooling to get features
        # x shape: (B, T', H', W', C)
        # We need to flatten to (B, N, C)
        features = x.flatten(1, 3)  # (B, T'*H'*W', C)

        # For Swin3D, we need to apply masking at the patch level
        # Create mask for original patch space
        _, mask, ids_restore = self.random_masking(x_patches, mask_ratio)

        # Use encoder features (may be downsampled)
        return features, mask, ids_restore, x_patches

    def forward_decoder(self, latent, ids_restore):
        """
        Forward through decoder.
        latent: (B, L, encoder_dim) - encoder output
        ids_restore: (B, N) - indices to restore order
        """
        B = latent.shape[0]

        # Project encoder features to decoder dimension
        x = self.decoder_embed(latent)  # (B, L, decoder_dim)

        # We need to expand to full patch space
        # For now, use a simpler approach: just decode the available features
        # This is a simplified MAE - in production, you'd want to handle masking better

        # Predict patches directly from available features
        pred = self.decoder_pred(x)  # (B, L', patch_dim)
        return pred

    def forward(self, x):
        """
        Full forward pass.
        x: (B, T, C, H, W)
        """
        # Encode with masking
        latent, mask, ids_restore, target = self.forward_encoder(x, self.mask_ratio)

        # Decode
        pred = self.forward_decoder(latent, ids_restore)

        return pred, target, mask

    def training_step(self, batch, batch_idx):
        x = batch

        # Forward pass
        pred, target, mask = self(x)

        # Compute loss on all patches (simplified)
        # In production, you'd only compute loss on masked patches
        loss = self.criterion(pred, target)

        self.log('train_loss', loss, prog_bar=True, logger=True, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x = batch

        # Forward pass
        pred, target, mask = self(x)

        # Compute loss
        loss = self.criterion(pred, target)

        self.log('val_loss', loss, prog_bar=True, sync_dist=True)

        # Save for visualization
        if batch_idx == 0:
            self.val_batch_for_viz = (target, pred)

        return loss

    def on_validation_epoch_end(self):
        if self.global_rank == 0 and hasattr(self, 'val_batch_for_viz'):
            target, pred = self.val_batch_for_viz
            try:
                visualize_reconstruction(target, pred, sample_idx=0, num_frames=16,
                                    epoch=self.current_epoch)
            except Exception as e:
                print(f"Could not save visualization: {e}")

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.hparams.lr,
            weight_decay=0.05,
            betas=(0.9, 0.95)
        )

        # Cosine scheduler with warmup
        cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.trainer.max_epochs
        )

        scheduler = GradualWarmupScheduler(
            optimizer,
            multiplier=1,
            total_epoch=5,
            after_scheduler=cosine_scheduler
        )

        return [optimizer], [scheduler]


def main():
    # ========== Data Loading ==========
    print("Loading datasets...")
    full_train_dataset = TileDataset(
        CFG.segment_path,
        splits=["train", "valid"],
        transform=get_transforms('train', CFG)
    )

    val_size = int(0.01 * len(full_train_dataset))
    train_size = len(full_train_dataset) - val_size

    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])

    train_loader = DataLoader(
        train_dataset,
        batch_size=CFG.train_batch_size,
        shuffle=True,
        num_workers=CFG.num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=CFG.valid_batch_size,
        shuffle=False,
        num_workers=CFG.num_workers,
        pin_memory=True
    )

    print(f"Train loader length: {len(train_loader)}")
    print(f"Val loader length: {len(val_loader)}")

    # ========== Model ==========
    print("Creating Swin3D MAE model...")
    model = Swin3DMAE(
        img_size=(16, 224, 224),
        patch_size=(2, 4, 4),
        in_channels=1,
        embed_dim=96,
        depths=(2, 2, 6, 2),
        num_heads=(3, 6, 12, 24),
        window_size=(4, 7, 7),
        decoder_embed_dim=512,
        decoder_depth=4,
        decoder_num_heads=8,
        mask_ratio=0.75,
        lr=1e-4
    )

    print(f"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    # ========== Training ==========
    checkpoint_callback = ModelCheckpoint(
        dirpath="checkpoints",
        filename="swin3d_mae_{epoch}",
        save_top_k=-1,
        every_n_epochs=1
    )

    trainer = pl.Trainer(
        max_epochs=CFG.epochs,
        accelerator="auto",
        devices=-1,
        strategy="ddp" if torch.cuda.device_count() > 1 else "auto",
        log_every_n_steps=20,
        check_val_every_n_epoch=1,
        gradient_clip_val=1.0,
        gradient_clip_algorithm="norm",
        callbacks=[checkpoint_callback],
        precision=16  # Mixed precision training
    )

    print("Starting training...")
    trainer.fit(model, train_loader, val_loader)


if __name__ == "__main__":
    main()
