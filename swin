"""
Swin3D Transformer for Vesuvius Ink Detection
Cleaned version - removed all duplicate class definitions
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import numpy as np
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
import segmentation_models_pytorch as smp
from torch.optim import AdamW
from torchvision.models.video import swin_transformer


class PatchExpanding3D(nn.Module):
    """3D Patch Expanding Layer (inverse of patch merging)"""
    def __init__(self, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.expand = nn.Linear(dim, 2 * dim, bias=False)
        self.norm = norm_layer(dim // 2)

    def forward(self, x):
        """
        x: B, T, H, W, C
        """
        B, T, H, W, C = x.shape
        x = self.expand(x)  # B, T, H, W, 2*C

        # Rearrange to upsample spatially
        x = x.view(B, T, H, W, 2, 2, C // 2)
        x = x.permute(0, 1, 2, 4, 3, 5, 6).contiguous()  # B, T, H, 2, W, 2, C//2
        x = x.view(B, T, H * 2, W * 2, C // 2)
        x = self.norm(x)
        return x


class SwinDecoderStage(nn.Module):
    """Decoder stage using Swin Transformer blocks from encoder"""
    def __init__(self, encoder_stage):
        super().__init__()
        # Clone the encoder stage blocks
        self.blocks = nn.Sequential(*[block for block in encoder_stage])

    def forward(self, x):
        return self.blocks(x)


class Swin3DEncoder(nn.Module):
    """Encapsulated Swin3D encoder with 1-channel input support"""
    def __init__(self, pretrained_ckpt=None, in_chans=1):
        super().__init__()
        # Load Swin3D backbone
        self.backbone = swin_transformer.swin3d_t(weights='KINETICS400_V1')

        # --- patch_embed adaptation for 1 channel ---
        old_conv = self.backbone.patch_embed.proj
        weight = old_conv.weight.sum(dim=1, keepdim=True)  # [out_channels, 1, kT, kH, kW]
        bias = old_conv.bias

        self.backbone.patch_embed.proj = nn.Conv3d(
            in_channels=in_chans,
            out_channels=old_conv.out_channels,
            kernel_size=old_conv.kernel_size,
            stride=old_conv.stride,
            bias=True
        )
        self.backbone.patch_embed.proj.weight = nn.Parameter(weight)
        self.backbone.patch_embed.proj.bias = nn.Parameter(bias.clone())

        # Remove classifier head, keep norm
        self.backbone.head = nn.Identity()

        # Load pretrained checkpoint if provided
        if pretrained_ckpt:
            self._load_pretrained_checkpoint(pretrained_ckpt)

    def _load_pretrained_checkpoint(self, ckpt_path):
        """Load pretrained MAE checkpoint"""
        print(f"Loading pretrained checkpoint from {ckpt_path}")
        try:
            import torch
            ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)
            if 'state_dict' in ckpt:
                state_dict = ckpt['state_dict']
            else:
                state_dict = ckpt

            # Filter encoder weights
            encoder_state = {}
            for k, v in state_dict.items():
                if k.startswith('encoder.'):
                    # Remove 'encoder.' prefix and adjust for backbone structure
                    new_key = k.replace('encoder.', 'backbone.')
                    if new_key.startswith('backbone.backbone.'):
                        new_key = new_key.replace('backbone.backbone.', 'backbone.')
                    encoder_state[new_key] = v

            missing, unexpected = self.backbone.load_state_dict(encoder_state, strict=False)
            print(f"Loaded pretrained weights. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        except Exception as e:
            print(f"Warning: Could not load pretrained checkpoint: {e}")

    def forward(self, x):
        x = self.backbone.patch_embed(x)   # (B, T/2, H/4, W/4, 96)
        x = self.backbone.pos_drop(x)

        skips = []
        # backbone.features = [stage0, PatchMerging, stage1, PatchMerging, stage2, PatchMerging, stage3]
        for block in self.backbone.features:
            x = block(x)
            if isinstance(block, nn.Sequential):  # stage output
                skips.append(x)

        return skips  # [stage0, stage1, stage2, stage3]


class SwinUNet3D(nn.Module):
    """Swin-UNet architecture adapted for 3D input (using Swin3D blocks in decoder)"""
    def __init__(self, num_classes=1, embed_dim=96, depths=[2, 2, 2, 2], num_heads=[3, 6, 12, 24]):
        super().__init__()
        self.num_classes = num_classes

        # ===== ENCODER =====
        self.encoder = Swin3DEncoder(in_chans=1)

        # Feature dimensions from Swin3D-tiny
        self.dims = [96, 192, 384, 768]

        # Extract encoder stages for reuse in decoder
        encoder_stages = []
        for block in self.encoder.backbone.features:
            if isinstance(block, nn.Sequential):
                encoder_stages.append(block)

        # ===== DECODER (Patch Expanding + Swin Transformer Blocks) =====
        self.decoder1_expand = PatchExpanding3D(dim=self.dims[3])
        self.decoder1_swin = SwinDecoderStage(encoder_stages[2])  # stage2 blocks

        self.decoder2_expand = PatchExpanding3D(dim=self.dims[2])
        self.decoder2_swin = SwinDecoderStage(encoder_stages[1])  # stage1 blocks

        # Optional: decoder3 for full resolution (currently commented out)
        # self.decoder3_expand = PatchExpanding3D(dim=self.dims[1])
        # self.decoder3_swin = SwinDecoderStage(encoder_stages[0])  # stage0 blocks

        # ===== SEGMENTATION HEAD (Transformer-based) =====
        final_dim = self.dims[1]  # 96
        self.segmentation_head = nn.Sequential(
            nn.LayerNorm(final_dim),
            nn.Linear(final_dim, final_dim),
            nn.GELU(),
            nn.Linear(final_dim, num_classes)
        )

    def forward(self, x):
        B, C, T, H, W = x.shape

        # ===== ENCODER =====
        skips = self.encoder(x)  # [stage0, stage1, stage2, stage3]
        x = skips[-1]  # bottleneck

        # ===== DECODER =====
        # stage3 (768) -> stage2 (384)
        x = self.decoder1_expand(x)
        x = x + skips[2]
        x = self.decoder1_swin(x)

        # stage2 (384) -> stage1 (192)
        x = self.decoder2_expand(x)
        x = x + skips[1]
        x = self.decoder2_swin(x)

        # Optional: stage1 (192) -> stage0 (96) for full resolution
        # x = self.decoder3_expand(x)
        # x = x + skips[0]
        # x = self.decoder3_swin(x)

        # Output at 1/4 resolution (H/4, W/4)
        # x shape: B, T, H/4, W/4, 96

        # ===== 3D → 2D =====
        # Temporal max pooling: B, T, H, W, C -> B, H, W, C
        x = x.max(dim=1)[0]

        # ===== SEGMENTATION HEAD =====
        # Apply per-pixel classification
        B, H, W, C = x.shape
        x = x.view(B * H * W, C)
        out = self.segmentation_head(x)
        out = out.view(B, H, W, self.num_classes)
        out = out.permute(0, 3, 1, 2)  # B, num_classes, H, W

        return out


class SwinModel(pl.LightningModule):
    """
    Swin3D Model for Vesuvius Ink Detection

    Args:
        pred_shape: Shape of prediction mask (H, W)
        size: Input tile size
        lr: Learning rate
        scheduler: Learning rate scheduler type
        wandb_logger: Weights & Biases logger
        freeze: Whether to freeze encoder
        checkpoint_path: Path to MAE pretrained checkpoint
        load_encoder_only: Only load encoder weights from checkpoint
    """
    def __init__(self, pred_shape, size, lr, scheduler=None, wandb_logger=None, freeze=False,
                 checkpoint_path=None, load_encoder_only=False):
        super().__init__()
        self.save_hyperparameters()

        # Prediction masks for validation
        self.mask_pred = np.zeros(self.hparams.pred_shape)
        self.mask_count = np.zeros(self.hparams.pred_shape)
        self.IGNORE_INDEX = 127

        # Losses
        self.loss_func1 = smp.losses.DiceLoss(mode='binary', smooth_factor=0.15, ignore_index=self.IGNORE_INDEX)
        self.loss_func2 = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.15, ignore_index=self.IGNORE_INDEX)
        self.loss_func = lambda x, y: 0.5 * self.loss_func1(x, y) + 0.5 * self.loss_func2(x, y)

        # Swin-Unet 3D backbone
        self.backbone = SwinUNet3D(
            num_classes=1,
            embed_dim=96,
            depths=[2, 2, 2, 2],
            num_heads=[3, 6, 12, 24]
        )

        # Load pretrained weights if provided
        if checkpoint_path:
            self.load_pretrained_weights(checkpoint_path, load_encoder_only)

        if freeze:
            self._freeze_encoder()

    def load_pretrained_weights(self, checkpoint_path, encoder_only=False):
        """
        Load pretrained weights from MAE checkpoint

        Args:
            checkpoint_path: Path to checkpoint file from mae-swin.py
            encoder_only: If True, only load encoder weights (recommended)
        """
        import torch
        print(f"Loading weights from {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

            # Handle different checkpoint formats
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint

            if encoder_only:
                # Load only encoder weights from MAE pretraining
                # MAE saves keys as: encoder.patch_embed.proj.weight
                # Swin3DEncoder expects: backbone.patch_embed.proj.weight
                encoder_state = {}
                for k, v in state_dict.items():
                    # Match keys from MAE encoder
                    if k.startswith('encoder.'):
                        # Remove 'encoder.' prefix and add 'backbone.' prefix
                        new_key = k.replace('encoder.', 'backbone.')
                        encoder_state[new_key] = v

                if encoder_state:
                    # Load into Swin3DEncoder's backbone
                    missing, unexpected = self.backbone.encoder.backbone.load_state_dict(
                        encoder_state, strict=False
                    )
                    print(f"✅ Loaded MAE encoder weights ({len(encoder_state)} keys)")
                    print(f"   Missing: {len(missing)}, Unexpected: {len(unexpected)}")

                    # Print some loaded keys for verification
                    loaded_keys = list(encoder_state.keys())[:5]
                    print(f"   Sample loaded keys: {loaded_keys}")
                else:
                    print("❌ Warning: No encoder weights found in checkpoint")
                    print(f"   Available keys in checkpoint: {list(state_dict.keys())[:10]}")
            else:
                # Load full model weights (not recommended for MAE checkpoints)
                missing, unexpected = self.load_state_dict(state_dict, strict=False)
                print(f"Loaded full model weights. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        except Exception as e:
            print(f"❌ Error loading checkpoint: {e}")
            import traceback
            traceback.print_exc()

    def _freeze_encoder(self):
        """Freeze encoder parameters"""
        for param in self.backbone.encoder.parameters():
            param.requires_grad = False
        print("Encoder frozen")

    def forward(self, x):
        """
        Forward pass
        Args:
            x: Input tensor of shape [B, C, T, H, W] where C=1
        Returns:
            Output tensor of shape [B, 1, H/4, W/4]
        """
        # Permute from [B, C, T, H, W] to [B, T, C, H, W] for Swin3D
        x = x.permute(0, 2, 1, 3, 4)
        output = self.backbone(x)
        return output

    def training_step(self, batch, batch_idx):
        """Training step"""
        x, y = batch
        outputs = self(x)
        loss = self.loss_func(outputs, y)

        if torch.isnan(loss):
            print("Warning: Loss is NaN")

        self.log("train/total_loss", loss.item(), on_step=True, on_epoch=True, prog_bar=True)

        opt = self.optimizers()
        current_lr = opt.param_groups[0]['lr']
        self.log("train/lr", current_lr, on_step=True, on_epoch=True, prog_bar=True)

        return {"loss": loss}

    def validation_step(self, batch, batch_idx):
        """Validation step"""
        x, y, xyxys = batch
        outputs = self(x)

        loss = self.loss_func(outputs, y)
        self.log("val/total_loss", loss.item(), on_step=True, on_epoch=True, prog_bar=True)

        # Accumulate predictions for full image reconstruction
        y_preds = torch.sigmoid(outputs).to('cpu')
        for i, (x1, y1, x2, y2) in enumerate(xyxys):
            # FIXED: scale_factor=4 to match 1/4 resolution output
            self.mask_pred[y1:y2, x1:x2] += F.interpolate(
                y_preds[i].unsqueeze(0).float(),
                scale_factor=4,  # FIXED: was 8
                mode='bilinear'
            ).squeeze(0).squeeze(0).numpy()
            self.mask_count[y1:y2, x1:x2] += np.ones((self.hparams.size, self.hparams.size))

        return {"loss": loss}

    def on_validation_epoch_end(self):
        """Called at the end of validation epoch"""
        mask_pred_tensor = torch.tensor(self.mask_pred, dtype=torch.float32, device=self.device)
        mask_count_tensor = torch.tensor(self.mask_count, dtype=torch.float32, device=self.device)

        if dist.is_available() and dist.is_initialized():
            dist.all_reduce(mask_pred_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(mask_count_tensor, op=dist.ReduceOp.SUM)

        if self.trainer.is_global_zero:
            mask_pred_np = mask_pred_tensor.cpu().numpy()
            mask_count_np = mask_count_tensor.cpu().numpy()
            final_mask = np.divide(
                mask_pred_np,
                mask_count_np,
                out=np.zeros_like(mask_pred_np),
                where=mask_count_np != 0
            )

            # Log to W&B if available
            if self.hparams.wandb_logger:
                self.hparams.wandb_logger.log_image(
                    key="masks",
                    images=[np.clip(final_mask, 0, 1)],
                    caption=["probs"]
                )

        # Reset accumulators
        self.mask_pred = np.zeros(self.hparams.pred_shape)
        self.mask_count = np.zeros(self.hparams.pred_shape)

    def configure_optimizers(self):
        """Configure optimizers and learning rate schedulers"""
        weight_decay = 0.
        base_lr = self.hparams.lr

        # All parameters in single group
        backbone_params = list(self.parameters())

        optimizer = AdamW(backbone_params, lr=base_lr, weight_decay=weight_decay)

        return [optimizer]


def load_weights(model, ckpt_path, strict=True, map_location='cpu'):
    """
    Load weights from a checkpoint into the model.

    Args:
        model: SwinModel instance
        ckpt_path: Path to the .ckpt file saved by PyTorch Lightning
        strict: Whether to strictly enforce that the keys in state_dict match
        map_location: Where to load the checkpoint (e.g., 'cpu', 'cuda')

    Returns:
        model: The model with loaded weights
    """
    import torch
    print(f"Loading checkpoint from: {ckpt_path}")

    ckpt = torch.load(ckpt_path, map_location=map_location, weights_only=False)

    # For Lightning checkpoints, weights are under 'state_dict'
    if 'state_dict' in ckpt:
        state_dict = ckpt['state_dict']
    else:
        state_dict = ckpt

    # Strip 'model.' prefix if saved with Lightning
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace("model.", "") if k.startswith("model.") else k
        new_state_dict[new_key] = v

    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=strict)

    print(f"Loaded checkpoint from: {ckpt_path}")
    print(f"Missing keys: {missing_keys}")
    print(f"Unexpected keys: {unexpected_keys}")

    return model

def scheduler_step(scheduler, avg_val_loss, epoch):
    scheduler.step(epoch) 
